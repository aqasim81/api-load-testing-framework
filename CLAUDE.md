# LoadForge — Development Guide

LoadForge — Python load testing framework with realistic traffic patterns,
a live React dashboard, and interactive HTML reports.

## Critical Rules

- **No AI attribution anywhere.** Never mention Claude, Claude Code, Anthropic, or any AI tool in commits, code, comments, docs, or any project artifact. No `Co-Authored-By` lines. No "generated by" notes.

## Current Status

- [x] Phase 0: Code quality infrastructure
- [x] Phase 1: Foundation + Scenario DSL
- [x] Phase 2: Traffic Patterns
- [x] Phase 3: Single-Worker Engine
- [x] Phase 4: Multi-Worker Distribution
- [ ] Phase 5: CLI Interface
- [ ] Phase 6: Post-Run Reports
- [ ] Phase 7: Live React Dashboard
- [ ] Phase 8: Polish, Documentation, Examples

## Stack

Core: Python 3.12+ (PEP 561 typed), `uv` package manager
Async: `aiohttp` + `uvloop` (asyncio fallback on Windows)
Multi-core: `multiprocessing` + `shared_memory`
CLI: `typer` + `rich` (Phase 5)
Dashboard: `FastAPI` + `uvicorn` backend, React 18 + Recharts frontend (Phase 7)
Reports: `plotly` + `jinja2` (Phase 6)
Stats: `numpy` + `hdrhistogram` (percentile-accurate metrics)
Quality: `ruff` (format + lint), `mypy` strict, `pytest` + `pytest-asyncio` (auto mode)

## Commands

```bash
uv sync --all-extras                    # Install all deps
make validate                           # Run ALL checks before committing
uv run ruff format src/ tests/          # Auto-format
uv run ruff check --fix src/ tests/     # Lint with auto-fix
uv run mypy src/                        # Type check
uv run pytest tests/unit/ -v --cov=loadforge --cov-branch --cov-fail-under=80
uv run pytest tests/integration/ -v     # Integration tests
```

## Architecture

- `src/loadforge/` — main package (PEP 561 typed)
- `_internal/` — config, errors, logging, types — NOT public API
- `dsl/` — scenario definition: `@scenario` and `@task` decorators, HTTP client with `RequestMetric`, scenario file loading
- `engine/` — load generation: scheduler creates virtual users, rate limiter throttles, session orchestrates full lifecycle
- `metrics/` — collects request metrics → `EndpointMetrics` → `MetricSnapshot` → `TestResult`
- `patterns/` — VU count generators over time (ramp, spike, step, diurnal, constant, composite)
- `dashboard/` — React frontend source (Phase 7); built assets → `src/loadforge/dashboard/static/`
- Tests mirror src structure: `tests/unit/`, `tests/integration/`, `tests/e2e/`

## Coding Conventions

- `from __future__ import annotations` at the top of EVERY file (ruff enforces)
- `X | Y` union syntax, never `Union` or `Optional`
- `Path` from pathlib, never `os.path`
- Google-style docstrings on all public classes/methods/functions with `Args:`, `Returns:`, `Raises:`
- Module-level docstrings on every non-`__init__.py` file

### Type Safety
- ZERO `Any` types — if a library returns `Any`, cast immediately
- ZERO bare `# type: ignore` — always specify error code: `# type: ignore[assignment]`
- All public functions/methods and dataclass fields MUST have full annotations

### Async Rules
- All I/O-bound operations MUST be async
- Never `time.sleep()` in async — use `asyncio.sleep()`
- Never blocking I/O in async — use `asyncio.to_thread()`
- Always `async with` for aiohttp sessions/responses
- Prefer `asyncio.TaskGroup` over `gather()`

### Error Handling
- Result types in business logic; never throw for expected failures
- Custom exceptions inherit from `LoadForgeError` (`_internal/errors.py`)
- Hierarchy: `LoadForgeError` → `ScenarioError` | `ConfigError` | `EngineError`
- Specific `except` blocks only; never bare `except Exception:` without re-raise

## Testing

- **80% minimum coverage** on business logic (enforced in `make validate` and CI)
- CI runs lint, type check, unit tests + coverage, integration tests on Python 3.12 and 3.13
- Unit tests: no I/O, no network, < 100ms each
- Integration tests: use echo server fixture from `conftest.py`
- Async tests: `async def test_...` — pytest-asyncio auto mode handles it
- Naming: `test_<function_name>_<scenario>`
- Hardcoded ports forbidden — use `_get_free_port()` from `conftest.py`

## Security

- Never hardcode secrets/keys/tokens — use environment variables via `_internal/config.py`
- Add new env vars to `.env.example` with a comment
- Gitleaks runs in pre-commit hooks and CI (full history scan)
- Tests must use fake credentials (e.g., `"test-token-12345"`)

## Git & Session Workflow

- Conventional commits: `feat:`, `fix:`, `chore:`, `docs:`, `test:`, `refactor:`
- One commit per logical change; run `make validate` before every commit
- Pre-commit hooks enforce formatting, linting, and secret scanning
- One task per conversation; use `/clear` between major tasks
- Commit after each working chunk; never run hours without committing

## References

`plans/implementation_plan.md` (architecture, risk register) | `plans/phases/` (phase details) | `plans/checklist.md` (progress) | `.github/workflows/ci.yml` (CI config)
